Platform Governance with Algorithm-based Content Moderation: An
Empirical Study on Reddit
Qinglai He1,*, Yili Hong2, T. S. Raghu3
Abstract
With increasing volumes of participation in social media and online communities, content moderation has
become an integral component of platform governance. Volunteer (human) moderators have thus far been
the essential workforce for content moderation. Because volunteer-based content moderation faces
challenges in achieving scalable, desirable, and sustainable moderation, many online platforms have
recently started to adopt algorithm-based content moderation tools (bots). When bots are introduced into
platform governance, it is unclear how volunteer moderators react in terms of their community-policing
and -nurturing efforts. 

                 To understand the impacts of these increasingly popular bot moderators, we conduct
jrhjkehh an empirical study with data collected from 156 communities (subreddits) on Reddit. Based on a
series of econometric analyses, we find that bots augment volunteer moderators by stimulating them to
moderate a larger quantity of posts, and such effects are pronounced in larger communities. Specifically,
volunteer moderators perform 20.9% more community policing, particularly over subjective rules.
Moreover, in communities with larger sizes, volunteers also exert increased efforts in offering more
explanations and suggestions after their community adopted bots. Notably, increases in activities are
primarily driven by the increased need for nurturing efforts to accompany growth in subjective policing.
Moreover, introducing bots to content moderation also improves the retention of volunteer moderators.
Overall, we show that introducing algorithm-based content moderation into platform governance is
beneficial for sustaining digital communities.
                                          Keywords: Content moderation, human-machine
collaboration, bot, volunteer moderators, platform governance
1 Wisconsin School of Business, University of Wisconsin–Madison, Madison, Wisconsin, 53706
2 Miami Herbert Business School, University of Miami, Coral Gables, Florida, 33146
3 W. P. Carey School of Business, Arizona State University, Tempe, Arizona, 85287
* Corresonding author. Email: qinglai.he@wisc.edu
1. Introduction
As online platforms and communities grow, inappropriate content such as hate speech and trolling pose
safety challenges and economic loss to platforms (Matias 2019a, Roberts 2014, Wu et al. 2021). Diverse
jrgejfghurehhrhjkhand complex online environments increase the need for devising platform policies and,
consequently, content moderation when content policies are violated. Typically, a small subset of
community members has served an important role as volunteer (human) content moderators. Platforms,
such as Reddit, Wikipedia, and Stack Overflow (Zheng et al. 2019), have largely relied on volunteers to
                             hmoderate their content and platform environment (Matias 2019a,
Jhaver et al. 2019b). With the increasing reach and influence of digital platforms, there is a growing
emphasis on more platform accountability. Volunteer moderators play an important role in meeting the
challenges of creating a safe environment for all users on digital platforms.
Based on the functionality of moderation tasks, volunteer moderators play community-policing
and -nurturing roles. Volunteers and platforms broadly face several challenges in content moderation. First,
content moderation is expected to scale in response to the fast-growing user base and the corresponding
growth in the volume of online content. It is, however, unrealistic to expect volunteer moderation capacity
to scale without effective automation support. Volunteer moderators’ time is not scalable, and they
frequently experience burnout in handling the large volume of online content and rule violations (Dosono
and Semaan 2019, Grimmelmann 2015, Gillespie 2018, Matias 2019b). Second, besides preventing the
community from being exposed to inappropriate content, volunteer moderators are also expected to nurture
community discourse, which emphasizes curating discussions through user education on community
norms and fostering improved dialogue with community members (Jiang 2020, Ruckenstein and Turunen
2020, Yu et al. 2018). Nurturing-oriented moderation requires more moderators’ effort and attention to
individuals’ needs than policing-oriented moderation. Motivating desirable nurturing moderation has been
a great challenge for online platforms. Last, as volunteer moderators engage in unpaid work, their retention
has been the key to the sustainability of the volunteerbased platform governance model. Platforms are
motivated to seek solutions that attract and sustain volunteers’ engagement and contributions and minimize
attrition.
Relevant academic research on human-bot collaboration in content moderation is still in its
infancy. First, IS researchers have long studied platform governance models (Sambamurthy and Zmud
1999, Tiwana et al. 2010) and content moderation (Kraut and Resnick 2012, Ren and Kraut 2014).
However, despite calls for research into the hybrid mode of content moderation with both human and
technological effort (Ren and Kraut 2014), few studies have empirically examined the influence of
machine adoption on volunteers’ moderation efforts in online platforms. Second, the machine substitution
and augmentation literature has investigated various contexts focusing predominantly on compensated
workers (Autor and Dorn 2013,
                                         Acemoglu and Restrepo 2018). As an emerging
context of great importance to modern society, content moderation has, however, received little attention,
particularly regarding the role of automation in volunteer-based work. Third, an emerging literature on
content moderation has started to provide qualitative and descriptive evidence on human and bot
performance (Jhaver et al. 2019a, Ruckenstein and Turunen 2020). These studies have laid the foundation
for our empirical investigation into the influences of bots on volunteers’ moderation.
In this study, we seek to investigate the impact of algorithm-powered bot moderators on volunteer
moderators’ activities. Formally, with the focus on two critical roles of volunteer moderators and the
challenges in content moderation (i.e., a growing need for more scalable and nurturing moderation), we
approach our research objectives by investigating two research questions: (RQ1) How do bots affect
volunteer moderators’ community-policing efforts? (RQ2) How do bots affect volunteer moderators’
community-nurturing efforts?
To answer the research questions, we select Reddit as our research context. The Reddit platform
has a plethora of communities named subreddits, with each having a small number of volunteer
moderators. To address the
                                                                               of the user base, Reddit
has developed bots (e.g., ‘AutoModerator’) to facilitate routine content moderation. Volunteer moderators
of subreddits can integrate the bots to assist in content moderation. Once integrated, when a bot detects a
rule violation on Reddit, it will follow the subreddit community guidelines and pre-defined workflow to
take actions such as flagging and removing content.
We collect bot and volunteer public moderation records from 156 subreddits on Reddit from 2013
to 2014. We identify the bot-automated moderation tasks from the public moderation records. To identify
volunteer moderators’ activities, we employ an advanced natural language processing technique, BERT
(Bidirectional Encoder Representations from Transformers). A Difference-in-Differences (DiD) model is
then applied to estimate the impact of moderation automation. Our results suggest that bots stimulate
volunteer moderators to increase their policing activities by 20.9%. Meanwhile, bots encourage volunteers
to exert more community-nurturing efforts. The number of explanations created by volunteers in a month
increases by 15.2% after adopting bots. And such effects are more pronounced in larger communities with
greater moderation needs. We find that bots augment rather than substitute volunteer moderators by
stimulating more of their policing efforts to enforce subjective rules and expand their scope of work.
Moreover, the increased community-nurturing efforts are primarily driven by the increasing need to
accompany policing activities over subjective rules. Additionally, volunteer-based moderation becomes
more sustainable with bots, as we observe increased retention of volunteer moderators after bot
implementation. We perform a series of robustness checks and validate the findings.
This research contributes to three streams of literature. Community-policing and -nurturing are interdependent. On the one hand, more policing can elicit
increasing needs for community-nurturing efforts, such as offering explanations and suggestions. On the
other hand, nurturing moderation complements policing-oriented moderation in achieving more effective
platform governance. For example, Jhaver et al. (2019c) found that while executing content removal,
offering sufficient explanations would significantly reduce the possibility of future rule violations.
In practice, community-nurturing moderation widely suffers from inadequate effort (Jiang 2020,
West 2018, Ruckenstein and Turunen 2020) compared to community-policing moderation because of the
greater cognitive complexity involved in nurturing-oriented activities. Moreover, existing literature
primarily focuses on policing-oriented moderation and its influence. Little attention has been paid to
volunteer moderators’ dual roles in content moderation and examining potential approaches to encourage
greater and more desirable community-nurturing efforts from moderators. We herein focus on whether and
how bot-assisted moderation may affect volunteer moderators’ efforts in policing- and nurturingoriented
moderation.
                                                  2.2 Bot Capability and Automation
Impacts
Online platforms increasingly rely on human and algorithmic agents (e.g., bots) in content moderation
(Ren and Kraut 2014). Extant literature has studied the capability of human and bot moderators (Seering et
al. 2018, Gillespie 2018, Jhaver et al. 2019a, Zheng et al. 2019). For example, Zheng et al. (2019) extract
bot functions based on bot description and activity history from Wikipedia and then categorize bots’ roles.
They found that bots play both protector and advisor roles on Wikipedia. The protector role is analogous to
the community-policing role, focusing on tasks such as identifying spam, vandals, and policy violations.
The advisor role is analogous to the community-nurturing role, focusing on tasks such as providing
suggestions for users and greeting newcomers. Notably, with a focus on bot usage in content moderation
on Reddit, Jhaver et al. (2019a) found that bots prove effective in replacing human moderators to remove
inappropriate content. Nonetheless, the study also revealed deficiencies in the bots’ ability to navigate
situations that demand an understanding of nuanced contextual details. As a result, human moderation
remains indispensable for enforcing community guidelines that necessitate subjective interpretation.
                            2.3 Bot Adoption and Community-policing Moderation
Drawing on the machine substitution and augmentation literature, the impact of bot adoption on
volunteers’ community-policing moderation efforts is heavily contingent upon task-specific
characteristics (Autor and Dorn 2013, Acemoglu and Restrepo 2018, Dixon et al. 2021). Volunteer
moderators exercise their policing decisions by following respective community guidelines. These
guidelines outline a series of community
                                    governing acceptable user-generated content, primarily focusing on content
format and quality standards (Fiesler et al. 2018, Seering et al. 2019). Considering the inherent
complexity of community rules, bots could exhibit varying capabilities in automating each rule,
leading to a diverse range of impacts on the workload of volunteer moderators.
In the case of simple and objective rules, bots possess the potential to efficiently automate related
policing tasks on a scalable basis, thereby reducing the reliance on human effort. For instance, research has
demonstrated that employing bots for policing simple and objective tasks can yield comparable
performance levels to those achieved by human moderators. Interviews with volunteer moderators on
Reddit found that bots can effectively identify certain forms of personal attacks and other inappropriate
content (Jhaver et al. 2019a). Follow-up research further reveals that when platform moderation maintains
high transparency, users perceive no distinction between removals carried out by human or bot moderators
(Jhaver et al. 2019c).
In contrast, when handling complex and subjective rules that involve more cognitive evaluation
and contextual judgment, bots exhibit limitations in displacing humans to exercise policing decisions
(Jhaver et al. 2019a, Seering et al. 2019). Complex community rules such as detecting satirical languages
and hate speech still largely rely on human judgment; thus, bots show limited capability in automating
policing-oriented moderation on subjective tasks. Moreover, as discussed earlier, when bots automate
simple and objective rules, volunteer moderators will have more bandwidth to enforce subjective rules.
Therefore, volunteer moderators’ moderation activities will not necessarily decrease after bots are
introduced to the community governance process.


